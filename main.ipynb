{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ghq1KP4AnmGA"
      },
      "source": [
        "![LogoUC3M](https://upload.wikimedia.org/wikipedia/commons/thumb/a/a6/Acr%C3%B3nimo_y_nombre_de_la_UC3M.svg/320px-Acr%C3%B3nimo_y_nombre_de_la_UC3M.svg.png)\n",
        "\n",
        "*Alonso Rios Guerra - 100495821 | Guillermo Sancho González - 100495991*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXynlq0Q41kQ"
      },
      "source": [
        "# *__Aprendizaje automático P1: Predicción del abandono de empleados__*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoWXNDVJnmGE"
      },
      "source": [
        "## *__1. Introducción__*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2B26xW_VnmGF"
      },
      "source": [
        "En esta práctica tenemos como objetivo desarrollar diferentes métodos de aprendizaje automático para predecir el abandono de los trabajadores de una empresa.\n",
        "\n",
        "Primero de todo empezaremos leyendo los datos que nos proporciona la empresa. En nuestro caso, usaremos el dataset Nº10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "sTFn1Cbi41kR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "data_path = 'attrition_availabledata_10.csv.gz'\n",
        "\n",
        "data = pd.read_csv(data_path, compression='gzip', sep = ',')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1CGR6A1nmGI"
      },
      "source": [
        "## *__2. EDA Simplificado__*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cw5J8UQRnmGI"
      },
      "source": [
        "Un EDA es una análisis exploratorio de datos, para organizar los datos, entender su contenido, entender cual son las variables más relevantes y cómo se relacionan unas con otras, determinar qué hacer con los datos faltantes y con los datos atípicos, y finalmente extraer conclusiones acerca de todo este análisis.\n",
        "\n",
        "Para hacer un eda debemos responder a distintas preguntas:\n",
        "\n",
        "-   ¿Cuántas instancias y atributos hay?\n",
        "\n",
        "-   ¿Qué tipo de atributos hay (numéricos o categóricos)? Esto se hace para verificar si hay características categóricas que deben ser codificadas (como variables dummy o one-hot encoding). Comprobar si hay variables categóricas con alta cardinalidad.\n",
        "\n",
        "-   ¿Qué atributos tienen valores faltantes y cuántos?\n",
        "\n",
        "-   ¿Existen columnas constantes o ID?\n",
        "\n",
        "-   ¿Es un problema de clasificación o regresión (variable de respuesta) y? En caso de clasificación, ¿las clases están desbalanceadas?\n",
        "\n",
        "A continuación le damos respuesta:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNxs_80mnmGJ"
      },
      "source": [
        "-   ¿Cuántas instancias y atributos hay?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "vRvk75z2nmGJ",
        "outputId": "71f21b29-619f-438d-c231-cdcabaaafdec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "La forma de la tabla es: (2940, 31)\n"
          ]
        }
      ],
      "source": [
        "print('La forma de la tabla es:', data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQwc8I4UnmGK"
      },
      "source": [
        "El dataset contiene 2940 instancias, 30 atributos y 1 etiqueta (Attrition)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SKjYFBGnmGL"
      },
      "source": [
        "-   ¿Qué tipo de atributos hay (numéricos o categóricos)? Esto se hace para verificar si hay características categóricas que deben ser codificadas (como variables dummy o one-hot encoding). Comprobar si hay variables categóricas con alta cardinalidad."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "QbRxpKDmnmGL",
        "outputId": "056c0de5-9f69-4782-9e67-cd06b2c865f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Los tipos de atributos son:\n",
            "==================================\n",
            "hrs                        float64\n",
            "absences                     int64\n",
            "JobInvolvement               int64\n",
            "PerformanceRating            int64\n",
            "EnvironmentSatisfaction    float64\n",
            "JobSatisfaction            float64\n",
            "WorkLifeBalance            float64\n",
            "Age                          int64\n",
            "BusinessTravel              object\n",
            "Department                  object\n",
            "DistanceFromHome             int64\n",
            "Education                    int64\n",
            "EducationField              object\n",
            "EmployeeCount                int64\n",
            "EmployeeID                   int64\n",
            "Gender                      object\n",
            "JobLevel                     int64\n",
            "JobRole                     object\n",
            "MaritalStatus               object\n",
            "MonthlyIncome                int64\n",
            "NumCompaniesWorked         float64\n",
            "Over18                      object\n",
            "PercentSalaryHike            int64\n",
            "StandardHours                int64\n",
            "StockOptionLevel             int64\n",
            "TotalWorkingYears          float64\n",
            "TrainingTimesLastYear        int64\n",
            "YearsAtCompany               int64\n",
            "YearsSinceLastPromotion      int64\n",
            "YearsWithCurrManager         int64\n",
            "Attrition                   object\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "print('Los tipos de atributos son:')\n",
        "print('==================================')\n",
        "print(data.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCo4CeNXnmGL"
      },
      "source": [
        "Existen dos tipos de atributos en nuestro dataset: numéricos y categóricos. Dentro de los numéricos encontramos de tipo entero (absences, Age, JobLevel, ...) y de tipo float (hrs, TotalWorkingYears, JobSatisfaction). En cuanto a lo atributos categóricos encontramos algunos como Department, JobRole, MaritalStatus, ... Para entrenar a nuestro modelo nos interesa codificar las variables categóricas y por ello es importante ver como de viable es según su cardinalidad."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "mx0MXq4enmGM",
        "outputId": "a613d211-2a2b-4c69-bd88-31206b8c7d05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cardinalidad de los atributos categóricos:\n",
            "================================\n",
            "BusinessTravel: 3 categorías únicas\n",
            "Department: 3 categorías únicas\n",
            "EducationField: 6 categorías únicas\n",
            "Gender: 2 categorías únicas\n",
            "JobRole: 9 categorías únicas\n",
            "MaritalStatus: 3 categorías únicas\n",
            "Over18: 1 categorías únicas\n",
            "Attrition: 2 categorías únicas\n"
          ]
        }
      ],
      "source": [
        "columnas_cat = data.select_dtypes(include=['object']).columns # Selecciona las columnas categóricas\n",
        "\n",
        "print('Cardinalidad de los atributos categóricos:')\n",
        "print('================================')\n",
        "for col in columnas_cat: # Imprime la cardinalidad de cada atributo categórico\n",
        "    print(f\"{col}: {data[col].nunique()} categorías únicas\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8WQOrQlnmGM"
      },
      "source": [
        "Al observar la ejecución del código anterior vemos que la cardinalidad de nuestros atributos categóricos es baja, en un rango de [2-9], y por ello no implicará ningún problema a la hora realizar una codificación dummy o One-Hot Encoding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrkKylOhnmGN"
      },
      "source": [
        "-   ¿Qué atributos tienen valores faltantes y cuántos?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "J8NdLkX4nmGN",
        "outputId": "1af18200-cfdf-4af7-e5a0-68657dff139e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cuántos valores faltan por atributo:\n",
            "====================================\n",
            "EnvironmentSatisfaction    15\n",
            "JobSatisfaction            12\n",
            "WorkLifeBalance            29\n",
            "NumCompaniesWorked         17\n",
            "TotalWorkingYears           5\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print('Cuántos valores faltan por atributo:')\n",
        "print('====================================')\n",
        "sin_valor = data.isnull().sum()  # Cuenta valores nulos por columna\n",
        "sin_valor = sin_valor[sin_valor > 0]  # Filtra solo los que tienen valores nulos\n",
        "\n",
        "print(sin_valor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zf7d4zjpnmGN"
      },
      "source": [
        "Tras ejecutar el código anterior, obtenemos que existen 5 atributos con valores faltantes. Estos atributos son:\n",
        "EnvironmentSatisfaction con 15 faltantes,\n",
        "JobSatisfaction con 12 faltantes,\n",
        "WorkLifeBalance con 29 faltantes,\n",
        "NumCompaniesWorked con 17 faltantes y\n",
        "TotalWorkingYears con 5 faltantes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbjKJjGAnmGO"
      },
      "source": [
        "- ¿Existen columnas constantes o ID?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "J_zmdAFlnmGO",
        "outputId": "d121e362-2e4a-4c24-c157-6869c6d24ffb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columnas constantes: ['EmployeeCount', 'Over18', 'StandardHours']\n",
            "Columnas ID: ['EmployeeID']\n"
          ]
        }
      ],
      "source": [
        "# 1. Comprobar columnas constantes\n",
        "constantes = [col for col in data.columns if data[col].nunique() == 1]\n",
        "print(\"Columnas constantes:\", constantes)\n",
        "\n",
        "# 2. Comprobar columnas ID\n",
        "columnas_id = [col for col in data.columns if data[col].nunique() == len(data)]\n",
        "print(\"Columnas ID:\", columnas_id)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GVB4x5enmGO"
      },
      "source": [
        "Observamos que existen 3 columnas constantes (EmployeeCount, Over18, StandardHours) y una columna ID (EmployeeID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOEP-cL9nmGP"
      },
      "source": [
        "- ¿Es un problema de clasificación o regresión (variable de respuesta) y? En caso de clasificación, ¿las clases están desbalanceadas?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-PZWflXnmGP"
      },
      "source": [
        "En este caso es fácil ver que es un problema de __clasificación__ porque la etiqueta (Attrition) en los datos train solo pueden tener valores 'Yes' o 'No', por lo que es una clase binaria."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "JiK1OJ0rnmGP",
        "outputId": "51f3a06c-8cd9-4429-d666-f9cb75fa0e41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comprobar si la clase está desbalanceada:\n",
            "======================================\n",
            "Attrition\n",
            "No     2466\n",
            "Yes     474\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Attrition\n",
            "No     0.838776\n",
            "Yes    0.161224\n",
            "Name: count, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "print('Comprobar si la clase está desbalanceada:')\n",
        "print('======================================')\n",
        "print(data['Attrition'].value_counts())\n",
        "print()\n",
        "print(data['Attrition'].value_counts() / data['Attrition'].count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwfMsvWtnmGQ"
      },
      "source": [
        "Se puede ver que la clase esta bastante desbalanceada: 83.88% No, 16.12% Yes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgWvcmNUnmGQ"
      },
      "source": [
        "## *__3. ¿Cómo se va a realizar la evaluación?__*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMNztO4lnmGQ"
      },
      "source": [
        "Para realizar la evaluación de nuestro modelo vamos a seguir una serie de pasos. La evaluación estará divida en inner, donde se elegirá el mejor classifier con el mejor scaler, imputer y ajuste de hiperparámetros, y outer, donde se estimará el rendimiento a futuro del modelo. Es importante dividir nuestro dataset en datos de entrenamiento (usados en inner) y de validación (usados en outer). En nuestro caso seguiremos el Holdout ((2/3) Train y (1/3) Test).\n",
        "\n",
        "- Inner Evaluation\n",
        "\n",
        "    - Primero, determinamos el mejor scaler e imputer. Para ello compararemos el score de hacer la cross-validation con 3 folds para KNN con los hiperparámetros por defecto variando el scaler (Standard, MinMax y Robust) y el imputer (Mean y Median).\n",
        "\n",
        "    - Una vez obtenido el mejor scaler e imputer, procederemos a buscar el mejor modelo. Para ello compararemos la precision de distintos modelos como KNN, Trees y Linear y SVM. Cada uno se comprobara con los hiper parámetros default y con los hiper parámetros optimizados.\n",
        "\n",
        "    - Con el mejor modelo elegido, pasaremos a la Outer Evaluation.\n",
        "\n",
        "- Outer Evaluation\n",
        "\n",
        "    - Con el mejor modelo obtenido en la fase de evaluación interna, validaremos su entrenamiento haciendo uso de la partición de datos TEST (1/3). Esto nos permitirá estimar el rendimiento del modelo elegido con vista a futuro.\n",
        "\n",
        "Además, una vez realizadas ambas evaluaciones, someteremos a nuestro modelo a unos datos de competición que nos devolverán ciertas predicciones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80W1LOkInmGQ"
      },
      "source": [
        "## *__4. Metodos básicos: KNN y Tree__*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTgw_PIpnmGQ"
      },
      "source": [
        "Inicialmente vamos a eliminar las columnas constantes o ids que consideramos que no aportan información útil al modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "uCZEhKLEnmGR"
      },
      "outputs": [],
      "source": [
        "data = data.drop(columns=constantes + columnas_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hEAqgVXnmGR"
      },
      "source": [
        "Se dividen los datos en x e y, donde x son los inputs, e y es la etiqueta\n",
        "\n",
        "Despues se vuelven a dividir en train y en test. Train contiene el 66% de los datos y se usará para la evaluación interna, y test contiene el 33% de los datos y se usará para la evaluación final. Los valores estarán estratificados porque las clases están muy desbalanceadas, es decir, la proporción de positivos y negativos en train y test será igual que la del conjunto de datos original.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "jfHykIMHnmGR"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "X = data.drop(columns=['Attrition'])\n",
        "y = data['Attrition']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/3, random_state=42, stratify= y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzK6CeOenmGS"
      },
      "source": [
        "Se itera creando pipelines con los distintos imputers y scalers para observar cual es el que tiene más precisión en el modelo.\n",
        "\n",
        "Se busca que haya una accuracy mínima de 0,8388 porque al estar las clases tan desbalanceadas esa sería la tasa de aciertos de un clasificador \"dummy\".\n",
        "En el caso de la balanced accuracy, se busca también que sea mayor a 0,5 que sería el valor del dummy classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "045usiZpnmGS",
        "outputId": "39f249ed-9388-41ed-f09b-b8fd72f7f7a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best balanced accuracy: 0.5954. With scaler: RobustScaler(), imputer: mean\n"
          ]
        }
      ],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler, RobustScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score, cross_val_predict\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "#Separar variables categoricas y numéricas\n",
        "columnas_num = X.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "columnas_cat = X.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "\n",
        "#Distintos métodos de escalado e imputación\n",
        "scalers = [StandardScaler(), MinMaxScaler(), RobustScaler()]\n",
        "imputers = ['mean', 'median']\n",
        "\n",
        "#Se realiza una crossvalidation estratificado con 3 folds\n",
        "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "accuracy = -np.inf\n",
        "best_scaler, best_imputer = None, None\n",
        "\n",
        "#Definir los pasos en la Pipeline\n",
        "\n",
        "knn = KNeighborsClassifier()\n",
        "encoder = OneHotEncoder()\n",
        "\n",
        "for i in range(len(imputers)):\n",
        "    for j in range(len(scalers)):\n",
        "        scaler = scalers[j]\n",
        "        imputer = SimpleImputer(strategy=imputers[i])\n",
        "\n",
        "        classif_numericos = Pipeline([\n",
        "            (\"imputation\", imputer),\n",
        "            (\"standardization\", scaler)\n",
        "        ])\n",
        "\n",
        "        classif_categoricos = Pipeline([\n",
        "            (\"encoder\", encoder),\n",
        "            (\"imputation\", imputer)\n",
        "        ])\n",
        "\n",
        "        preprocessor = ColumnTransformer(\n",
        "            transformers=[\n",
        "                (\"num\", classif_numericos, columnas_num),\n",
        "                (\"cat\", classif_categoricos, columnas_cat)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        clf = Pipeline([(\"preprocessor\", preprocessor), (\"classifier\", knn)])\n",
        "\n",
        "        clf.fit(X_train, y_train)\n",
        "\n",
        "        y_pred = cross_val_predict(clf, X_train, y_train, cv=cv)\n",
        "\n",
        "        tn, fp, fn, tp = confusion_matrix(y_train, y_pred).ravel()\n",
        "\n",
        "        # Calculamos TPR, TNR, Accuracy y Balanced Accuracy \n",
        "        tpr = tp / (tp + fn)\n",
        "        tnr = tn / (tn + fp)\n",
        "\n",
        "        acc = (tp + tn) / (tp + fp + tn + fn)\n",
        "        bal_acc = (tpr + tnr) / 2\n",
        "\n",
        "        if bal_acc > accuracy:\n",
        "            accuracy = bal_acc\n",
        "            best_scaler, best_imputer = scalers[j], imputers[i]\n",
        "\n",
        "        \"\"\"print(f\"Matriz de confusión: tpr: {tpr}, tnr: {tnr}\")\n",
        "        print(f\"Balanced accuracy: {bal_acc:.4f}\")\n",
        "        print(f\"Accuracy: {acc:.4f}\")\"\"\"\n",
        "\n",
        "print(f\"Best balanced accuracy: {accuracy:.4f}. With scaler: {best_scaler}, imputer: {best_imputer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjAr9-zXnmGT"
      },
      "source": [
        "El mejor scaler e imputer son ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SR8khA2nmGT"
      },
      "source": [
        "Una vez sabemos cual es el mejor scaler y el mejor imputer, hacemos el mismo proceso hecho antes, pero para elegir los mejores hiperparámetros del KNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "a1_V5d0EnmGT",
        "outputId": "e6a53400-d32a-4049-d9d6-6b6413c2b860"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best balanced accuracy: 0.7874. With n: 1, p: 1\n"
          ]
        }
      ],
      "source": [
        "classif_numericos = Pipeline([\n",
        "    (\"imputation\", SimpleImputer(strategy=best_imputer)),\n",
        "    (\"standardization\", best_scaler)\n",
        "])\n",
        "\n",
        "classif_categoricos = Pipeline([\n",
        "    (\"encoder\", OneHotEncoder()),\n",
        "    (\"imputation\", SimpleImputer(strategy=best_imputer))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", classif_numericos, columnas_num),\n",
        "        (\"cat\", classif_categoricos, columnas_cat)\n",
        "    ]\n",
        ")\n",
        "accuracy = -np.inf\n",
        "best_n, best_p = 0, 0\n",
        "\n",
        "for n in [1, 2, 3, 4, 8, 16, 32]: # Elegir el numero de neighbors\n",
        "    for p in [1, 2]: # Usar la distancia de Manhattan (1) o Euclídea (2)\n",
        "\n",
        "        clf = Pipeline([(\"preprocessor\", preprocessor), (\"classifier\", KNeighborsClassifier(p=p, n_neighbors=n))])\n",
        "\n",
        "        y_pred = cross_val_predict(clf, X_train, y_train, cv=cv)\n",
        "\n",
        "        tn, fp, fn, tp = confusion_matrix(y_train, y_pred).ravel()\n",
        "\n",
        "        # Calculamos TPR, TNR, Accuracy y Balanced Accuracy \n",
        "        tpr = tp / (tp + fn)\n",
        "        tnr = tn / (tn + fp)\n",
        "\n",
        "        bal_acc = (tpr + tnr) / 2\n",
        "        acc = (tp + tn) / (tp + fp + tn + fn)\n",
        "        \n",
        "        if bal_acc > accuracy:\n",
        "            accuracy = bal_acc\n",
        "            best_n, best_p = n, p\n",
        "        \n",
        "        \"\"\"print(f\"Matriz de confusión: tpr: {tpr}, tnr: {tnr}\")\n",
        "        print(f\"Balanced accuracy: {bal_acc:.4f}\")\n",
        "        print(f\"Accuracy: {acc:.4f}\")\"\"\"\n",
        "\n",
        "print(f\"Best balanced accuracy: {accuracy:.4f}. With n: {best_n}, p: {best_p}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSzpFOFsnmGU"
      },
      "source": [
        "Los hiperparámetros optimizados obtenidos son que el valor de K (n-neighbors) sea 1 y se utilize la distancia de Manhattan (p=1).\n",
        "\n",
        "Se observa que cuantos menos neighbors haya, mayor sera el TPR y menor será el TNR, pero cuantos más neighbors haya, será al revés.\n",
        "Como la clase minoritaria es la positiva, estamos más intereresados en los que acierten más ésta, es decir, mayor TPR.\n",
        "El problema es que si se selecciona un valor de k muy bajo, el modelo se ha sobreadaptado a los datos train, y habrá overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Decision Tree con los hiperparámetros por defecto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matriz de confusión: tpr: 0.6234177215189873, tnr: 0.9148418491484185\n",
            "Balanced accuracy: 0.7691\n",
            "Accuracy: 0.8679\n"
          ]
        }
      ],
      "source": [
        "from sklearn import tree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "clf = Pipeline([(\"preprocessor\", preprocessor), (\"classifier\", tree.DecisionTreeClassifier())])\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "#Para ver el arbol \n",
        "\n",
        "\"\"\"import matplotlib.pyplot as plt\n",
        "fig = plt.figure(figsize=(200,200))\n",
        "_ = tree.plot_tree(clf.named_steps[\"classifier\"],\n",
        "                feature_names = clf.named_steps[\"preprocessor\"].get_feature_names_out(),\n",
        "                class_names= ['No', 'Yes'],\n",
        "                filled=True)\"\"\"\n",
        "\n",
        "\n",
        "y_pred = cross_val_predict(clf, X_train, y_train, cv=cv)\n",
        "\n",
        "tn, fp, fn, tp = confusion_matrix(y_train, y_pred).ravel()\n",
        "\n",
        "# Calculamos TPR, TNR, Accuracy y Balanced Accuracy \n",
        "tpr = tp / (tp + fn)\n",
        "tnr = tn / (tn + fp)\n",
        "\n",
        "bal_acc = (tpr + tnr) / 2\n",
        "acc = (tp + tn) / (tp + fp + tn + fn)\n",
        "\n",
        "print(f\"Matriz de confusión: tpr: {tpr}, tnr: {tnr}\")\n",
        "print(f\"Balanced accuracy: {bal_acc:.4f}\")\n",
        "print(f\"Accuracy: {acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ajuste de hiperparámetros para el arbol:\n",
        "\n",
        "Primero elegimos el criterio utilizado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matriz de confusión: tpr: 0.620253164556962, tnr: 0.9136253041362531\n",
            "Balanced accuracy: 0.7669\n",
            "Accuracy: 0.8663\n",
            "Matriz de confusión: tpr: 0.5727848101265823, tnr: 0.9184914841849149\n",
            "Balanced accuracy: 0.7456\n",
            "Accuracy: 0.8628\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for criterion in [\"gini\", \"entropy\"]:\n",
        "    clf = Pipeline([(\"preprocessor\", preprocessor), (\"classifier\", tree.DecisionTreeClassifier(criterion=criterion))])\n",
        "\n",
        "    clf.fit(X_train,y_train)\n",
        "    y_pred = cross_val_predict(clf, X_train, y_train, cv=cv)\n",
        "\n",
        "    tn, fp, fn, tp = confusion_matrix(y_train, y_pred).ravel()\n",
        "\n",
        "    # Calculamos TPR, TNR, Accuracy y Balanced Accuracy \n",
        "    tpr = tp / (tp + fn)\n",
        "    tnr = tn / (tn + fp)\n",
        "\n",
        "    bal_acc = (tpr + tnr) / 2\n",
        "    acc = (tp + tn) / (tp + fp + tn + fn)\n",
        "\n",
        "    print(f\"Matriz de confusión: tpr: {tpr}, tnr: {tnr}\")\n",
        "    print(f\"Balanced accuracy: {bal_acc:.4f}\")\n",
        "    print(f\"Accuracy: {acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El mejor criterio es gini\n",
        "\n",
        "Comprobando la profundidad máxima:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matriz de confusión: tpr: 0.0, tnr: 1.0\n",
            "Balanced accuracy: 0.5000\n",
            "Accuracy: 0.8388\n",
            "Matriz de confusión: tpr: 0.23734177215189872, tnr: 0.9458637469586375\n",
            "Balanced accuracy: 0.5916\n",
            "Accuracy: 0.8316\n",
            "Matriz de confusión: tpr: 0.26582278481012656, tnr: 0.944647201946472\n",
            "Balanced accuracy: 0.6052\n",
            "Accuracy: 0.8352\n",
            "Matriz de confusión: tpr: 0.44936708860759494, tnr: 0.9361313868613139\n",
            "Balanced accuracy: 0.6927\n",
            "Accuracy: 0.8577\n",
            "Matriz de confusión: tpr: 0.6107594936708861, tnr: 0.916058394160584\n",
            "Balanced accuracy: 0.7634\n",
            "Accuracy: 0.8668\n",
            "Matriz de confusión: tpr: 0.620253164556962, tnr: 0.9105839416058394\n",
            "Balanced accuracy: 0.7654\n",
            "Accuracy: 0.8638\n"
          ]
        }
      ],
      "source": [
        "for max_depth in [1, 2, 4, 8, 16, None]:\n",
        "    clf = Pipeline([(\"preprocessor\", preprocessor), (\"classifier\", tree.DecisionTreeClassifier(max_depth=max_depth))])\n",
        "\n",
        "    clf.fit(X_train,y_train)\n",
        "    y_pred = cross_val_predict(clf, X_train, y_train, cv=cv)\n",
        "\n",
        "    tn, fp, fn, tp = confusion_matrix(y_train, y_pred).ravel()\n",
        "\n",
        "    # Calculamos TPR, TNR, Accuracy y Balanced Accuracy \n",
        "    tpr = tp / (tp + fn)\n",
        "    tnr = tn / (tn + fp)\n",
        "\n",
        "    bal_acc = (tpr + tnr) / 2\n",
        "    acc = (tp + tn) / (tp + fp + tn + fn)\n",
        "\n",
        "    print(f\"Matriz de confusión: tpr: {tpr}, tnr: {tnr}\")\n",
        "    print(f\"Balanced accuracy: {bal_acc:.4f}\")\n",
        "    print(f\"Accuracy: {acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Aparentemente, cuanto mayor sea la profundidad máxima, mejor precisión tendrá el modelo.\n",
        "\n",
        "Comprobamos los efectos de modificar min_samples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matriz de confusión: tpr: 0.6234177215189873, tnr: 0.9081508515815085\n",
            "Balanced accuracy: 0.7658\n",
            "Accuracy: 0.8622\n",
            "Matriz de confusión: tpr: 0.5316455696202531, tnr: 0.9081508515815085\n",
            "Balanced accuracy: 0.7199\n",
            "Accuracy: 0.8474\n",
            "Matriz de confusión: tpr: 0.4588607594936709, tnr: 0.916058394160584\n",
            "Balanced accuracy: 0.6875\n",
            "Accuracy: 0.8423\n",
            "Matriz de confusión: tpr: 0.43670886075949367, tnr: 0.920316301703163\n",
            "Balanced accuracy: 0.6785\n",
            "Accuracy: 0.8423\n",
            "Matriz de confusión: tpr: 0.2563291139240506, tnr: 0.9562043795620438\n",
            "Balanced accuracy: 0.6063\n",
            "Accuracy: 0.8434\n"
          ]
        }
      ],
      "source": [
        "for min_samples in [2, 10, 20, 30, 100]:\n",
        "    clf = Pipeline([(\"preprocessor\", preprocessor), (\"classifier\", tree.DecisionTreeClassifier(min_samples_split=min_samples))])\n",
        "\n",
        "    clf.fit(X_train,y_train)\n",
        "    y_pred = cross_val_predict(clf, X_train, y_train, cv=cv)\n",
        "\n",
        "    tn, fp, fn, tp = confusion_matrix(y_train, y_pred).ravel()\n",
        "\n",
        "    # Calculamos TPR, TNR, Accuracy y Balanced Accuracy \n",
        "    tpr = tp / (tp + fn)\n",
        "    tnr = tn / (tn + fp)\n",
        "\n",
        "    bal_acc = (tpr + tnr) / 2\n",
        "    acc = (tp + tn) / (tp + fp + tn + fn)\n",
        "\n",
        "    print(f\"Matriz de confusión: tpr: {tpr}, tnr: {tnr}\")\n",
        "    print(f\"Balanced accuracy: {bal_acc:.4f}\")\n",
        "    print(f\"Accuracy: {acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El mejor valor para min_samples será el que está por defecto, 2.\n",
        "\n",
        "Finalmente, revisemos otro hiperparámetro llamado min_impurity_decrease: esto significa que solo se crea un nuevo nivel del árbol si la ganancia de información (es decir, la disminución de entropía o Gini) es mayor que min_impurity_decrease. Es otra forma de controlar la profundidad del árbol."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "With min_impurity_decrease 0.0: 0.84\n",
            "With min_impurity_decrease 0.2222222222222222: 0.84\n",
            "With min_impurity_decrease 0.4444444444444444: 0.84\n",
            "With min_impurity_decrease 0.6666666666666666: 0.84\n",
            "With min_impurity_decrease 0.8888888888888888: 0.84\n",
            "With min_impurity_decrease 1.1111111111111112: 0.84\n",
            "With min_impurity_decrease 1.3333333333333333: 0.84\n",
            "With min_impurity_decrease 1.5555555555555554: 0.84\n",
            "With min_impurity_decrease 1.7777777777777777: 0.84\n",
            "With min_impurity_decrease 2.0: 0.84\n"
          ]
        }
      ],
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "for min_impurity_decrease in np.linspace(0,2,num=10):\n",
        "    Pipeline([(\"preprocessor\", preprocessor), (\"classifier\", tree.DecisionTreeClassifier(min_impurity_decrease=min_impurity_decrease))])\n",
        "    clf.fit(X_train,y_train)\n",
        "    y_test_pred = clf.predict(X_test)\n",
        "    accuracy_tree = metrics.accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "    print(f\"With min_impurity_decrease {min_impurity_decrease}: {accuracy_tree:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La modificación de éste valor parece no modificar mucho el modelo."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
